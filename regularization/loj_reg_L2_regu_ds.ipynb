{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=3, n_redundant=0, n_repeated=0, n_classes=2, flip_y=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X: {X.shape}, shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X setinin ilk 5 elemanı:\")\n",
    "print(X[:5])\n",
    "print(\" \")\n",
    "print(\"y setinin ilk 5 elemanı:\")\n",
    "print(y[:5])\n",
    "print(\" \")\n",
    "print(\"X setinin en büyük ve en küçük değerleri:\")\n",
    "print(np.max(X), np.min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.zeros((X.shape[0], X.shape[1] + 5))\n",
    "X_new[:, 0] = X[:, 0]\n",
    "X_new[:, 1] = X[:, 1]\n",
    "X_new[:, 2] = X[:, 2]\n",
    "X_new[:, 3] = X[:, 0] * X[:, 0]\n",
    "X_new[:, 4] = X[:, 1] * X[:, 1] * X[:, 1]\n",
    "X_new[:, 5] = X[:, 2] * X[:, 2]\n",
    "X_new[:, 6] = X[:, 0] * X[:, 1]\n",
    "X_new[:, 7] = X[:, 0] * X[:, 1] * X[:, 2]\n",
    "\n",
    "print(X_new[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.95, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.num_samples = 0\n",
    "        self.num_features = 0\n",
    "        self.bias = None\n",
    "        self.losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "    def loss(self, y_true, y_predicted):\n",
    "        log_probs = y_true * np.log(y_predicted) + (1 - y_true) * (np.log(1 - y_predicted))\n",
    "        return (-1.0 / self.num_samples) * np.sum(log_probs)\n",
    "\n",
    "    def fit(self, X, y, X_test, y_test):\n",
    "        self.num_samples, self.num_features = X.shape\n",
    "        self.weights = np.zeros(self.num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / self.num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / self.num_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights = self.weights - (self.lr * dw)\n",
    "            self.bias = self.bias - (self.lr * db)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                self.losses.append(self.loss(y, y_predicted))\n",
    "                # calculate test set loss\n",
    "                linear_model_validation = np.dot(X_test, self.weights) + self.bias\n",
    "                y_predicted_validation = sigmoid(linear_model_validation)\n",
    "                self.validation_losses.append(self.loss(y_test, y_predicted_validation))\n",
    "\n",
    "        return self.losses, self.validation_losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        y_predicted_classes = np.where(y_predicted > 0.5, 1, 0)\n",
    "        return y_predicted_classes\n",
    "\n",
    "    def accuracy(self, y_true, y_predicted):\n",
    "        return np.sum(y_true == y_predicted) / len(y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "NUM_ITERS = int(2e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LogisticRegression(lr=LEARNING_RATE, n_iters=NUM_ITERS)\n",
    "loss, validation_set_loss = regressor.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(range(len(loss)), loss, label=\"Train loss\")\n",
    "plt.plot(range(len(loss)), validation_set_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"Iterations (x20)\", fontsize=18)\n",
    "plt.ylabel(r\"$J(\\theta)$\", fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = regressor.predict(X_train)\n",
    "print(f\"Train accuracy: {regressor.accuracy(y_train, y_train_pred)}\")\n",
    "\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "print(f\"Test accuracy: {regressor.accuracy(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionRegularized:\n",
    "    def __init__(self, lr=0.001, lambd=10, n_iters=400):\n",
    "        self.lr = lr\n",
    "        self.lambd = lambd\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.num_samples = 0\n",
    "        self.num_features = 0\n",
    "        self.bias = None\n",
    "        self.losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "    def loss(self, y_true, y_predicted):\n",
    "        log_probs = y_true * np.log(y_predicted) + (1 - y_true) * (np.log(1 - y_predicted))\n",
    "        cross_entropy_cost = (-1.0 / self.num_samples) * np.sum(log_probs)\n",
    "        L2_regularization_cost = (self.lambd / (2 * self.num_samples)) * np.sum(np.square(self.weights))\n",
    "        return cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "    def fit(self, X, y, X_test, y_test):\n",
    "        self.num_samples, self.num_features = X.shape\n",
    "        self.weights = np.zeros(self.num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = sigmoid(linear_model)\n",
    "\n",
    "            db = (1 / self.num_samples) * np.sum(y_predicted - y)\n",
    "            dw = (1 / self.num_samples) * np.dot(X.T, (y_predicted - y)) + (self.lambd / self.num_samples) * self.weights\n",
    "\n",
    "            self.bias = self.bias - (self.lr * db)\n",
    "            self.weights = self.weights - (self.lr * dw)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                self.losses.append(self.loss(y, y_predicted))\n",
    "                # calculate test set loss\n",
    "                linear_model_validation = np.dot(X_test, self.weights) + self.bias\n",
    "                y_predicted_validation = sigmoid(linear_model_validation)\n",
    "                self.validation_losses.append(self.loss(y_test, y_predicted_validation))\n",
    "\n",
    "        return self.losses, self.validation_losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        y_predicted_classes = np.where(y_predicted > 0.5, 1, 0)\n",
    "        return y_predicted_classes\n",
    "\n",
    "    def accuracy(self, y_true, y_predicted):\n",
    "        return np.sum(y_true == y_predicted) / len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_regularized = LogisticRegressionRegularized(lr=LEARNING_RATE, lambd=10, n_iters=NUM_ITERS)\n",
    "losses_regularized, validation_loss_regularized = regressor_regularized.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(range(len(losses_regularized)), losses_regularized, label=\"Train loss r.\")\n",
    "plt.plot(range(len(losses_regularized)), validation_loss_regularized, label=\"Validation loss r.\")\n",
    "plt.xlabel(\"Iterations (x20)\", fontsize=18)\n",
    "plt.ylabel(r\"$J(\\theta)$ Regularized\", fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_train_pred_regularized = regressor_regularized.predict(X_train)\n",
    "print(f\"Train accuracy: {regressor_regularized.accuracy(y_train, y_train_pred_regularized)}\")\n",
    "\n",
    "y_test_pred_regularized = regressor_regularized.predict(X_test)\n",
    "print(f\"Test accuracy: {regressor_regularized.accuracy(y_test, y_test_pred_regularized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot(range(len(losses_regularized)), losses_regularized, label=\"Regularized\", color=\"coral\")\n",
    "plt.plot(range(len(validation_loss_regularized)), validation_loss_regularized, label=\"Val Regularized\", color=\"red\")\n",
    "\n",
    "plt.plot(range(len(loss)), loss, label=\"Not regularized\", color=\"blue\")\n",
    "plt.plot(range(len(validation_set_loss)), validation_set_loss, label=\"Val Not Regularized\", color=\"teal\")\n",
    "\n",
    "plt.xlabel(\"Iterations (x20)\", fontsize=18)\n",
    "plt.ylabel(r\"$J(\\theta)$\", fontsize=18)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b25c72524e0bc61a78b3d6a2df84de3adaea47dfccaeebc487f09611983c18d9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
